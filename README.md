# End-To-End-Streaming-Big-Data
## End-To-End Streaming Big Data Project makes processing big data easy with Airflow, Kafka, Spark, Apache Hive and much more!!

## Top Contents:
+ Streaming Big Amount of Data using Kafka and SparkStreaming.
+ Managing Apache Kafka with Control Center, Apache Zookeeper and Schema Registry.
+ Snowflake Data Warehouse 
+ Automated Medallion Architecture using Data Orchestration Tools (Apache Airflow)
+ Processing Data Lake using DeltaLake, Object Storage with MinIO compatible with AWS S3.
+ Third-party query engine Snowflake for High Query Performance.
+ Data Visualization Tools with Superset.
+ Project Report.

## Dataset:
This project uses fake created fact data related to e-commerce platform while streaming data with Kafka.

## Schema Model
![schema_model](https://github.com/user-attachments/assets/4727ee2f-8403-4c20-b473-b9a28553ca9b)

## Tools & Technologies
+ Streaming Data Process: Apache Kafka, Apache Spark.
+ Data Warehouse: Snowflake
+ IDE: Pycharm
+ Programming Languages: Python.
+ Data Orchestration Tool: Apache Airflow
+ Data Lake/ Data Lakehouse: DeltaLake, MinIO
+ Data Visualization Tool: Superset
+ Containerization: Docker, Docker Compose.

## Architecture
![System-Architecture](https://github.com/user-attachments/assets/d7a2d426-8d96-4ca8-9097-03f95f71c53d)

## Setup
### Pre-requisites: 
First, you'll your Pycharm IDE, Docker, Apache Kafka, Apache Spark and Apache Airflow setup in your project.

### How can I make this better?!
A lot can still be done :)
+ Choose managed Infra
  + Cloud Composer for Airflow, Kafka and Spark using AWS.
+ Kafka Streaming process monitering with Prometheus and Grafana.
+ Include CI/CD Operations.
+ OLAP Operations for higher query performance with data warehouse.
+ Write data quality tests.
+ Storage Layer Deployment with AWS S3 and Terraform.
